{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Cast Tutorial\n",
    "Set-up instructions: this notebook give a tutorial on the high-dimensional time series forecasting task supported by U-Cast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install Python 3.10. For convenience, execute the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conda env create -f environment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. U-Cast Construction\n",
    "\n",
    "U-Cast is an efficient model that captures channel correlations via learning latent hierarchical structures. U-Cast also introduces a full-rank regularization\n",
    "term to encourage disentanglement and improve the learning of structured representations.\n",
    "\n",
    "In the following section, we will have a detailed view on U-Cast. To make it clearer, please see the figures below.\n",
    "\n",
    "![U-Cast](../pic/U-Cast.png)\n",
    "\n",
    "U-Cast consist of three main component (1) downsampling (2) full-rank regularization (3) upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling (HierarchicalLatentQueryNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class HierarchicalLatentQueryNetwork(nn.Module):\n",
    "    def __init__(self, orig_channels, time_dim, num_layers, head_dim, reduction_ratio=16, num_heads=1, dropout=0.1):\n",
    "        super(HierarchicalLatentQueryNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        latent_dims = []\n",
    "        current_channels = orig_channels\n",
    "        for _ in range(num_layers):\n",
    "            new_channels = max(1, current_channels // reduction_ratio)\n",
    "            latent_dims.append(new_channels)\n",
    "            current_channels = new_channels\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        current_in_dim = time_dim\n",
    "        for latent_dim in latent_dims:\n",
    "            self.layers.append(LatentQueryAttention(current_in_dim, latent_dim, head_dim, num_heads, dropout))\n",
    "            current_in_dim = head_dim * num_heads\n",
    "        self.norm_layers = nn.ModuleList([nn.LayerNorm(head_dim * num_heads) for _ in latent_dims])\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, T, C = x.shape\n",
    "        x_base = x.transpose(1, 2)  # [B, C, T]\n",
    "        skip_list = [x_base]\n",
    "        x_down = x_base\n",
    "        attn_maps = [] if return_attn else None\n",
    "        for layer, norm in zip(self.layers, self.norm_layers):\n",
    "            if return_attn:\n",
    "                x_down, attn = layer(x_down, return_attn=True)\n",
    "                attn_maps.append(attn.detach().cpu())\n",
    "            else:\n",
    "                x_down = layer(x_down)\n",
    "            x_down = norm(x_down)\n",
    "            skip_list.append(x_down)\n",
    "        if return_attn:\n",
    "            return skip_list[-1], skip_list, attn_maps\n",
    "        else:\n",
    "            return skip_list[-1], skip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LatentQueryAttention(nn.Module):\n",
    "    def __init__(self, in_dim, latent_dim, head_dim, num_heads=1, dropout=0.1):\n",
    "        super(LatentQueryAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.latent_queries = nn.Parameter(torch.randn(latent_dim, head_dim * num_heads))\n",
    "        self.q_proj = nn.Linear(head_dim * num_heads, head_dim * num_heads)\n",
    "        self.k_proj = nn.Linear(in_dim, head_dim * num_heads)\n",
    "        self.v_proj = nn.Linear(in_dim, head_dim * num_heads)\n",
    "        self.out_proj = nn.Linear(head_dim * num_heads, head_dim * num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, L, _ = x.shape\n",
    "        queries = self.latent_queries.unsqueeze(0).expand(B, -1, -1)\n",
    "        queries = self.q_proj(queries)\n",
    "        keys = self.k_proj(x)\n",
    "        values = self.v_proj(x)\n",
    "        queries = queries.view(B, self.latent_dim, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, self.latent_dim, self.num_heads * self.head_dim)\n",
    "        out = self.out_proj(out)\n",
    "        if return_attn:\n",
    "            return out, attn\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full-rank regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def covariance_loss(skip_list, lambda_cov=0.1, eps=1e-5):\n",
    "    \"\"\"\n",
    "    skip_list: list of tensors, each tensor is of shape [B, C, D]\n",
    "    Computes a normalized sum of negative log-determinants of covariance matrices.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_layers = len(skip_list) - 1  # exclude input\n",
    "    for x in skip_list[1:]:\n",
    "        B, C, D = x.shape\n",
    "        x_reshaped = x.reshape(B * C, D)\n",
    "        x_centered = (x_reshaped - x_reshaped.mean(dim=0, keepdim=True)) / (\n",
    "            x_reshaped.std(dim=0, keepdim=True) + eps\n",
    "        )\n",
    "        cov = (x_centered.T @ x_centered) / (B * C - 1)\n",
    "        cov = cov + eps * torch.eye(D, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # normalize by dimension (D) to reduce scale variance\n",
    "        loss = -torch.logdet(cov) / D\n",
    "        total_loss += loss\n",
    "\n",
    "    return lambda_cov * (total_loss / num_layers if num_layers > 0 else 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class HierarchicalUpsamplingNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, q_in_dim, head_dim, num_heads=1, dropout=0.1):\n",
    "        super(HierarchicalUpsamplingNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            UpLatentQueryAttention(q_in_dim, head_dim, num_heads, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norms = nn.ModuleList([\n",
    "            nn.LayerNorm(q_in_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x_bottom, skip_list):\n",
    "        rev = list(reversed(skip_list))\n",
    "        queries = rev[1:]\n",
    "        x = x_bottom\n",
    "        for layer, norm, query in zip(self.layers, self.norms, queries):\n",
    "            x = norm(layer(query, x) + query)\n",
    "            # x = layer(query, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "For more details, please read the our paper (link: https://arxiv.org/pdf/2507.15119)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. U-Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@register_model(\"UCast\", paper=\"U-Cast: Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting\", year=2024)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "\n",
    "    def forecast(self, x_enc):\n",
    "\n",
    "    def forward(self, x_enc):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let us focus on __init__(self, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def __init__(self, configs):\n",
    "    super(Model, self).__init__()\n",
    "    self.task_name = configs.task_name\n",
    "    self.seq_len = configs.seq_len\n",
    "    self.pred_len = configs.pred_len\n",
    "    self.enc_in = configs.enc_in\n",
    "    self.d_model = configs.d_model\n",
    "    self.alpha = configs.alpha\n",
    "\n",
    "    self.input_proj = nn.Linear(self.seq_len, self.d_model)\n",
    "    self.output_proj = nn.Linear(self.d_model, self.pred_len)\n",
    "\n",
    "    self.channel_reduction_net = HierarchicalLatentQueryNetwork(\n",
    "        orig_channels=self.enc_in,\n",
    "        time_dim=self.d_model,\n",
    "        num_layers=configs.e_layers,\n",
    "        head_dim=self.d_model,\n",
    "        reduction_ratio=configs.channel_reduction_ratio,\n",
    "        num_heads=1,\n",
    "        dropout=configs.dropout\n",
    "    )\n",
    "\n",
    "    self.upsample_net = HierarchicalUpsamplingNetwork(\n",
    "        num_layers=configs.e_layers,\n",
    "        q_in_dim=self.d_model,\n",
    "        head_dim=self.d_model,\n",
    "        num_heads=1,\n",
    "        dropout=configs.dropout\n",
    "    )\n",
    "\n",
    "    self.predict_layer = nn.Linear(self.d_model, self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Then, let's focus on forecast(self, x_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def forecast(self, x_enc):\n",
    "    means = x_enc.mean(1, keepdim=True).detach()\n",
    "    x_enc = x_enc - means\n",
    "    stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "    x_enc = x_enc / stdev\n",
    "\n",
    "    x_enc = x_enc.transpose(1, 2)  # [B, C, T]\n",
    "    x_enc = self.input_proj(x_enc)  # [B, C, d_model]\n",
    "    x_enc = x_enc.transpose(1, 2)  # [B, d_model, C]\n",
    "    x_bottom, skip_list = self.channel_reduction_net(x_enc)\n",
    "    cov_loss = covariance_loss(skip_list, self.alpha)\n",
    "    x_bottom = self.predict_layer(x_bottom)\n",
    "    x_up = self.upsample_net(x_bottom, skip_list)\n",
    "    dec_out = self.output_proj(x_up + x_enc.transpose(1, 2))  # [B, enc_in, pred_len]\n",
    "    dec_out = dec_out.transpose(1, 2)  # [B, pred_len, enc_in]\n",
    "\n",
    "    dec_out = dec_out * stdev[:, 0, :].unsqueeze(1)\n",
    "    dec_out = dec_out + means[:, 0, :].unsqueeze(1)\n",
    "    return dec_out, cov_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def forward(self, x_enc):\n",
    "    return self.forecast(x_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Training for Hign-dimensional Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# class LongTermForecastingExperiment(BaseExperiment)\n",
    "def train(self, setting: str) -> Tuple[nn.Module, list, dict, str]:\n",
    "    \"\"\"\n",
    "    Execute the complete training procedure.\n",
    "    \n",
    "    Performs model training with validation monitoring, early stopping,\n",
    "    and comprehensive metric tracking across all epochs.\n",
    "    \n",
    "    Args:\n",
    "        setting: Unique experiment setting string for checkpoint management\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - all_epoch_metrics: List of metrics for each training epoch\n",
    "            - best_metrics: Dictionary of best validation metrics achieved\n",
    "            - best_model_path: Path to the saved best model checkpoint\n",
    "    \"\"\"\n",
    "    # Load data splits\n",
    "    train_data, train_loader = self._get_data(flag='train')\n",
    "    vali_data, vali_loader = self._get_data(flag='val')\n",
    "    test_data, test_loader = self._get_data(flag='test')\n",
    "    \n",
    "    # Setup checkpoint directory\n",
    "    path = os.path.join(self.config.checkpoints, setting)\n",
    "    \n",
    "    # Initialize performance tracking variables\n",
    "    all_epoch_metrics = []\n",
    "    best_metrics = {\n",
    "        \"epoch\": 0,\n",
    "        \"train_loss\": float('inf'),\n",
    "        \"vali_loss\": float('inf'),\n",
    "        \"vali_mae_loss\": float('inf'),\n",
    "        \"test_loss\": float('inf'),\n",
    "        \"test_mae_loss\": float('inf')\n",
    "    }\n",
    "    best_model_path = \"\"\n",
    "    \n",
    "    time_now = time.time()\n",
    "    \n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=self.config.patience, verbose=True, accelerator=self.accelerator)\n",
    "    \n",
    "    # Prepare components for distributed training with accelerator\n",
    "    self.model, self.optimizer, train_loader, vali_loader, test_loader = self.accelerator.prepare(\n",
    "        self.model, self.optimizer, train_loader, vali_loader, test_loader\n",
    "    )\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(self.config.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        \n",
    "        self.model.train()\n",
    "        epoch_time = time.time()\n",
    "        batch_times = []  # Track training time per batch\n",
    "        \n",
    "        # Batch training loop\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()  # Record batch start time\n",
    "            \n",
    "            iter_count += 1\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Move data to device\n",
    "            batch_x = batch_x.float().to(self.device)\n",
    "            batch_y = batch_y.float().to(self.device)\n",
    "            batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "            \n",
    "            # Prepare decoder input (for sequence-to-sequence models)\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -self.config.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :self.config.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "            \n",
    "            # Forward pass with automatic mixed precision\n",
    "            with self.accelerator.autocast():\n",
    "                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            \n",
    "            # Handle models that return additional loss components\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, additional_loss = outputs\n",
    "            else:\n",
    "                additional_loss = 0\n",
    "            \n",
    "            # Calculate loss (only on prediction horizon)\n",
    "            batch_y = batch_y[:, -self.config.pred_len:, :].to(self.device)\n",
    "            loss = self.criterion(outputs, batch_y) + additional_loss\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            # Log progress every 100 iterations\n",
    "            if (i + 1) % 100 == 0:\n",
    "                self.accelerator.print(f\"\\titers: {i+1}, epoch: {epoch+1} | loss: {loss.item():.7f}\")\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((self.config.train_epochs - epoch) * train_steps - i)\n",
    "                self.accelerator.print(f'\\tspeed: {speed:.4f}s/iter; left time: {left_time:.4f}s')\n",
    "                iter_count = 0\n",
    "                time_now = time.time()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            self.accelerator.backward(loss)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            batch_end_time = time.time()\n",
    "            batch_times.append(batch_end_time - batch_start_time)  # Record batch training time\n",
    "        \n",
    "        # Calculate epoch timing statistics\n",
    "        epoch_cost_time = time.time() - epoch_time\n",
    "        avg_batch_time = np.mean(batch_times)\n",
    "        self.accelerator.print(f\"Epoch: {epoch+1} cost time: {epoch_cost_time:.2f}s\")\n",
    "        self.accelerator.print(f\"Average batch training time: {avg_batch_time:.4f}s\")\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        train_loss = np.average(train_loss)\n",
    "        val_time = time.time()\n",
    "        vali_loss, vali_mae_loss = self.validate(vali_loader)\n",
    "        self.accelerator.print(f\"Val cost time: {time.time() - val_time:.2f}s\")\n",
    "        test_time = time.time()\n",
    "        test_loss, test_mae_loss = self.validate(test_loader)\n",
    "        self.accelerator.print(f\"Test cost time: {time.time() - test_time:.2f}s\")\n",
    "        \n",
    "        # Record comprehensive epoch metrics\n",
    "        epoch_metrics = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"vali_loss\": float(vali_loss),\n",
    "            \"vali_mae_loss\": float(vali_mae_loss),\n",
    "            \"test_loss\": float(test_loss),\n",
    "            \"test_mae_loss\": float(test_mae_loss)\n",
    "        }\n",
    "        all_epoch_metrics.append(epoch_metrics)\n",
    "        \n",
    "        self.accelerator.print(f'Epoch: {epoch+1}, Steps: {train_steps} | Train Loss: {train_loss:.7f} Vali Loss: {vali_loss:.7f} Test Loss: {test_loss:.7f}')\n",
    "        \n",
    "        # Early stopping check (includes saving checkpoint)\n",
    "        early_stopping(vali_loss, self.model, path, metrics=epoch_metrics)\n",
    "\n",
    "        # Update best metrics if current model is better\n",
    "        if vali_loss < best_metrics[\"vali_loss\"]:\n",
    "            best_metrics.update(epoch_metrics)\n",
    "            best_model_path = early_stopping.get_checkpoint_path()\n",
    "\n",
    "        # Stop if needed\n",
    "        if early_stopping.early_stop:\n",
    "            self.accelerator.print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        # Adjust learning rate according to schedule\n",
    "        adjust_learning_rate(self.optimizer, epoch + 1, self.config, self.accelerator)\n",
    "    \n",
    "    return all_epoch_metrics, best_metrics, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more, please see it at core/experiments/long_term_forecasting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare components for distributed training with accelerator\n",
    "self.model, self.optimizer, train_loader, vali_loader, test_loader = self.accelerator.prepare(\n",
    "    self.model, self.optimizer, train_loader, vali_loader, test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, accelerator=None):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.accelerator = accelerator\n",
    "        self.best_metrics = None  # Store best validation metrics\n",
    "\n",
    "    def __call__(self, val_loss, model, path, metrics=None):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path, metrics)\n",
    "            if metrics is not None:\n",
    "                self.best_metrics = metrics\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.accelerator:\n",
    "                self.accelerator.print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            else:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path, metrics)\n",
    "            if metrics is not None:\n",
    "                self.best_metrics = metrics\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, args, accelerator=None):\n",
    "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif args.lradj == 'type3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 3 else args.learning_rate * (0.9 ** ((epoch - 3) // 1))}\n",
    "    elif args.lradj == 'constant':\n",
    "        lr_adjust = {epoch: args.learning_rate}\n",
    "    elif args.lradj == 'TSLR':\n",
    "        lr_adjust = {epoch: args.learning_rate * ((0.5 ** 0.1) ** (epoch // 20))}\n",
    "    elif args.lradj == '3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate * 0.1}\n",
    "    elif args.lradj == '4':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate * 0.1}\n",
    "    elif args.lradj == '5':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate * 0.1}\n",
    "    elif args.lradj == '6':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate * 0.1}\n",
    "    elif args.lradj == 'TST':\n",
    "        lr_adjust = {epoch: args.learning_rate * (1.0 + 0.1 * epoch / args.train_epochs)}\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        if accelerator is not None:\n",
    "            accelerator.print('Updating learning rate to {}'.format(lr))\n",
    "        else:\n",
    "            print('Updating learning rate to {}'.format(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def validate(self, vali_loader=None) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Validate the model using distributed metric aggregation to avoid GPU OOM.\n",
    "\n",
    "    Args:\n",
    "        vali_loader: Optional DataLoader for validation data. If None, it will be created internally.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (MSE, MAE)\n",
    "    \"\"\"\n",
    "    if vali_loader is None:\n",
    "        _, vali_loader = self._get_data(flag='val')\n",
    "\n",
    "    # Initialize distributed accumulators\n",
    "    sum_sq_error = torch.tensor(0.0, device=self.device)\n",
    "    sum_abs_error = torch.tensor(0.0, device=self.device)\n",
    "    total_count = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in vali_loader:\n",
    "            # Move data to device\n",
    "            batch_x = batch_x.float().to(self.device)\n",
    "            batch_y = batch_y.float().to(self.device)\n",
    "            batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "            # Prepare decoder input\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -self.config.pred_len:, :])\n",
    "            dec_inp = torch.cat(\n",
    "                [batch_y[:, :self.config.label_len, :], dec_inp], dim=1\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            with self.accelerator.autocast():\n",
    "                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            # Slice true values\n",
    "            true_slice = batch_y[:, -self.config.pred_len:, :]\n",
    "\n",
    "            # Compute batch errors\n",
    "            error = outputs - true_slice\n",
    "            sum_sq_error += error.pow(2).sum()\n",
    "            sum_abs_error += error.abs().sum()\n",
    "            total_count += torch.tensor(error.numel(), device=self.device)\n",
    "\n",
    "    # Reduce metrics across all devices once\n",
    "    sum_sq_error = self.accelerator.reduce(sum_sq_error, reduction=\"sum\")\n",
    "    sum_abs_error = self.accelerator.reduce(sum_abs_error, reduction=\"sum\")\n",
    "    total_count = self.accelerator.reduce(total_count, reduction=\"sum\")\n",
    "\n",
    "    # Compute final metrics\n",
    "    mse = sum_sq_error / total_count\n",
    "    mae = sum_abs_error / total_count\n",
    "\n",
    "    self.model.train()\n",
    "    return mse.item(), mae.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test(self, setting: str, best_model_path: Optional[str] = None) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on test data using the same aggregation logic as in validate().\n",
    "\n",
    "    Args:\n",
    "        setting: Experiment identifier string, used for result saving and fallback checkpoint loading.\n",
    "        best_model_path: Path to the model checkpoint. If None, defaults to ./checkpoints/{setting}.pth\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (MSE, MAE) on the test set.\n",
    "    \"\"\"\n",
    "    # Determine checkpoint path\n",
    "    if best_model_path is None:\n",
    "        best_model_path = os.path.join(self.config.checkpoints, f\"{setting}.pth\")\n",
    "\n",
    "    self.accelerator.print(f'Loading trained model {best_model_path} for testing')\n",
    "    \n",
    "    # Load model weights\n",
    "    self.model = self.accelerator.unwrap_model(self.model)\n",
    "    self.model.load_state_dict(torch.load(best_model_path, map_location='cpu'))\n",
    "\n",
    "    # Get test data loader and prepare for distributed eval\n",
    "    _, test_loader = self._get_data(flag='test')\n",
    "    self.model, test_loader = self.accelerator.prepare(self.model, test_loader)\n",
    "\n",
    "    # Use validate() to compute MSE and MAE\n",
    "    mse, mae = self.validate(test_loader)\n",
    "\n",
    "    self.accelerator.print(f'Test MSE: {mse:.6f}, Test MAE: {mae:.6f}')\n",
    "    return mse, mae"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
