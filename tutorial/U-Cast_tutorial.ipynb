{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Cast Tutorial\n",
    "This notebook give a tutorial on the high-dimensional time series forecasting task supported by U-Cast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install Python 3.10. For convenience, execute the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conda env create -f environment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. U-Cast Construction\n",
    "\n",
    "U-Cast is an efficient model that captures channel correlations via learning latent hierarchical structures. U-Cast also introduces a full-rank regularization\n",
    "term to encourage disentanglement and improve the learning of structured representations.\n",
    "\n",
    "In the following section, we will have a detailed view on U-Cast. To make it clearer, please see the figures below.\n",
    "\n",
    "![U-Cast](../pic/U-Cast.png)\n",
    "\n",
    "U-Cast consist of three main component (1) downsampling (2) full-rank regularization (3) upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling (HierarchicalLatentQueryNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class HierarchicalLatentQueryNetwork(nn.Module):\n",
    "    def __init__(self, orig_channels, time_dim, num_layers, head_dim, reduction_ratio=16, num_heads=1, dropout=0.1):\n",
    "        super(HierarchicalLatentQueryNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Compute latent dimensions for each layer, progressively reducing channels\n",
    "        latent_dims = []\n",
    "        current_channels = orig_channels\n",
    "        for _ in range(num_layers):\n",
    "            new_channels = max(1, current_channels // reduction_ratio)\n",
    "            latent_dims.append(new_channels)\n",
    "            current_channels = new_channels\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        current_in_dim = time_dim\n",
    "        \n",
    "        # Build hierarchical attention layers\n",
    "        for latent_dim in latent_dims:\n",
    "            self.layers.append(LatentQueryAttention(current_in_dim, latent_dim, head_dim, num_heads, dropout))\n",
    "            current_in_dim = head_dim * num_heads\n",
    "        self.norm_layers = nn.ModuleList([nn.LayerNorm(head_dim * num_heads) for _ in latent_dims])\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, T, C = x.shape\n",
    "        x_base = x.transpose(1, 2)  # [B, C, T] - convert to channel-first format\n",
    "        skip_list = [x_base]  # Save skip connections for upsampling\n",
    "        x_down = x_base\n",
    "        attn_maps = [] if return_attn else None\n",
    "        \n",
    "        # Downsample layer by layer, learning hierarchical representations\n",
    "        for layer, norm in zip(self.layers, self.norm_layers):\n",
    "            if return_attn:\n",
    "                x_down, attn = layer(x_down, return_attn=True)\n",
    "                attn_maps.append(attn.detach().cpu())\n",
    "            else:\n",
    "                x_down = layer(x_down)\n",
    "            x_down = norm(x_down)\n",
    "            skip_list.append(x_down)  # Save output of each layer for upsampling\n",
    "            \n",
    "        if return_attn:\n",
    "            return skip_list[-1], skip_list, attn_maps\n",
    "        else:\n",
    "            return skip_list[-1], skip_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Latent Query Attention Mechanism - Channel reduction via learnable query vectors\n",
    "class LatentQueryAttention(nn.Module):\n",
    "    def __init__(self, in_dim, latent_dim, head_dim, num_heads=1, dropout=0.1):\n",
    "        super(LatentQueryAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Learnable latent query vectors, used to extract important information from high-dimensional input\n",
    "        self.latent_queries = nn.Parameter(torch.randn(latent_dim, head_dim * num_heads))\n",
    "        self.q_proj = nn.Linear(head_dim * num_heads, head_dim * num_heads)\n",
    "        self.k_proj = nn.Linear(in_dim, head_dim * num_heads)\n",
    "        self.v_proj = nn.Linear(in_dim, head_dim * num_heads)\n",
    "        self.out_proj = nn.Linear(head_dim * num_heads, head_dim * num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, L, _ = x.shape\n",
    "        # Expand latent queries to batch dimension\n",
    "        queries = self.latent_queries.unsqueeze(0).expand(B, -1, -1)\n",
    "        queries = self.q_proj(queries)\n",
    "        keys = self.k_proj(x)\n",
    "        values = self.v_proj(x)\n",
    "        \n",
    "        # Reshape into multi-head attention format\n",
    "        queries = queries.view(B, self.latent_dim, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores and weights\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention weights and generate output\n",
    "        out = torch.matmul(attn, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, self.latent_dim, self.num_heads * self.head_dim)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        if return_attn:\n",
    "            return out, attn\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full-rank regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Full-rank regularization loss - Encourages learning disentangled representations\n",
    "def covariance_loss(skip_list, lambda_cov=0.1, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Compute the normalized sum of negative log-determinant of covariance matrices \n",
    "    for full-rank regularization.\n",
    "    \n",
    "    Args:\n",
    "        skip_list: list of tensors, each tensor has shape [B, C, D]\n",
    "        lambda_cov: scaling factor for covariance loss\n",
    "        eps: small constant for numerical stability\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_layers = len(skip_list) - 1  # Exclude input layer\n",
    "    \n",
    "    # Compute covariance loss for each skip connection layer\n",
    "    for x in skip_list[1:]:\n",
    "        B, C, D = x.shape\n",
    "        x_reshaped = x.reshape(B * C, D)\n",
    "        \n",
    "        # Normalize features\n",
    "        x_centered = (x_reshaped - x_reshaped.mean(dim=0, keepdim=True)) / (\n",
    "            x_reshaped.std(dim=0, keepdim=True) + eps\n",
    "        )\n",
    "        \n",
    "        # Compute covariance matrix\n",
    "        cov = (x_centered.T @ x_centered) / (B * C - 1)\n",
    "        cov = cov + eps * torch.eye(D, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # Dimension normalization to reduce scale variance\n",
    "        loss = -torch.logdet(cov) / D  # Negative log-determinant encourages full rank\n",
    "        total_loss += loss\n",
    "\n",
    "    return lambda_cov * (total_loss / num_layers if num_layers > 0 else 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hierarchical Upsampling Network - U-Cast upsampling component, restores original dimensions using skip connections\n",
    "class HierarchicalUpsamplingNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, q_in_dim, head_dim, num_heads=1, dropout=0.1):\n",
    "        super(HierarchicalUpsamplingNetwork, self).__init__()\n",
    "        # Build upsampling attention layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            UpLatentQueryAttention(q_in_dim, head_dim, num_heads, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        # Layer normalization\n",
    "        self.norms = nn.ModuleList([\n",
    "            nn.LayerNorm(q_in_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x_bottom, skip_list):\n",
    "        # Reverse skip connection list to upsample from bottom to top\n",
    "        rev = list(reversed(skip_list))\n",
    "        queries = rev[1:]  # Exclude the bottom-most layer\n",
    "        x = x_bottom\n",
    "        \n",
    "        # Upsample layer by layer, using skip connections as queries\n",
    "        for layer, norm, query in zip(self.layers, self.norms, queries):\n",
    "            x = norm(layer(query, x) + query)  # Residual connection\n",
    "            # x = layer(query, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, please read the our paper (link: https://arxiv.org/pdf/2507.15119)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. U-Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@register_model(\"UCast\", paper=\"U-Cast: Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting\", year=2024)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "\n",
    "    def forecast(self, x_enc):\n",
    "\n",
    "    def forward(self, x_enc):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let us focus on __init__(self, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def __init__(self, configs):\n",
    "    super(Model, self).__init__()\n",
    "    self.task_name = configs.task_name\n",
    "    self.seq_len = configs.seq_len\n",
    "    self.pred_len = configs.pred_len\n",
    "    self.enc_in = configs.enc_in\n",
    "    self.d_model = configs.d_model\n",
    "    self.alpha = configs.alpha\n",
    "\n",
    "    # Project input sequence length into model dimension\n",
    "    self.input_proj = nn.Linear(self.seq_len, self.d_model)\n",
    "    # Project model outputs back to prediction horizon\n",
    "    self.output_proj = nn.Linear(self.d_model, self.pred_len)\n",
    "\n",
    "    # Encoder: hierarchical latent query network for channel reduction\n",
    "    self.channel_reduction_net = HierarchicalLatentQueryNetwork(\n",
    "        orig_channels=self.enc_in,\n",
    "        time_dim=self.d_model,\n",
    "        num_layers=configs.e_layers,\n",
    "        head_dim=self.d_model,\n",
    "        reduction_ratio=configs.channel_reduction_ratio,\n",
    "        num_heads=1,\n",
    "        dropout=configs.dropout\n",
    "    )\n",
    "\n",
    "    # Decoder: hierarchical upsampling network to reconstruct channels\n",
    "    self.upsample_net = HierarchicalUpsamplingNetwork(\n",
    "        num_layers=configs.e_layers,\n",
    "        q_in_dim=self.d_model,\n",
    "        head_dim=self.d_model,\n",
    "        num_heads=1,\n",
    "        dropout=configs.dropout\n",
    "    )\n",
    "\n",
    "    # Final prediction head\n",
    "    self.predict_layer = nn.Linear(self.d_model, self.d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's focus on forecast(self, x_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def forecast(self, x_enc):\n",
    "    # Normalize input: zero-mean and unit-variance per channel\n",
    "    means = x_enc.mean(1, keepdim=True).detach()\n",
    "    x_enc = x_enc - means\n",
    "    stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "    x_enc = x_enc / stdev\n",
    "\n",
    "    # Project input into model space\n",
    "    x_enc = x_enc.transpose(1, 2)        # [B, C, T]\n",
    "    x_enc = self.input_proj(x_enc)       # [B, C, d_model]\n",
    "    x_enc = x_enc.transpose(1, 2)        # [B, d_model, C]\n",
    "\n",
    "    # Encoder: channel reduction with hierarchical latent queries\n",
    "    x_bottom, skip_list = self.channel_reduction_net(x_enc)\n",
    "    cov_loss = covariance_loss(skip_list, self.alpha)\n",
    "\n",
    "    # Bottleneck prediction transformation\n",
    "    x_bottom = self.predict_layer(x_bottom)\n",
    "\n",
    "    # Decoder: upsample using skip connections\n",
    "    x_up = self.upsample_net(x_bottom, skip_list)\n",
    "\n",
    "    # Final projection to prediction horizon\n",
    "    dec_out = self.output_proj(x_up + x_enc.transpose(1, 2))  # [B, enc_in, pred_len]\n",
    "    dec_out = dec_out.transpose(1, 2)  # [B, pred_len, enc_in]\n",
    "\n",
    "    # Denormalize output\n",
    "    dec_out = dec_out * stdev[:, 0, :].unsqueeze(1)\n",
    "    dec_out = dec_out + means[:, 0, :].unsqueeze(1)\n",
    "\n",
    "    return dec_out, cov_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def forward(self, x_enc):\n",
    "    return self.forecast(x_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Training for Hign-dimensional Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# class LongTermForecastingExperiment(BaseExperiment)\n",
    "def train(self, setting: str) -> Tuple[nn.Module, list, dict, str]:\n",
    "    \"\"\"\n",
    "    Execute the complete training procedure with validation monitoring and early stopping.\n",
    "\n",
    "    Args:\n",
    "        setting: Unique experiment setting string for checkpoint management\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - all_epoch_metrics: List of metrics for each training epoch\n",
    "            - best_metrics: Dictionary of best validation metrics achieved\n",
    "            - best_model_path: Path to the saved best model checkpoint\n",
    "    \"\"\"\n",
    "    # Load data splits (train/val/test loaders may include padding and masking)\n",
    "    train_data, train_loader = self._get_data(flag='train')\n",
    "    vali_data, vali_loader = self._get_data(flag='val')\n",
    "    test_data, test_loader = self._get_data(flag='test')\n",
    "    \n",
    "    # Checkpoint directory for early-stopped best model\n",
    "    path = os.path.join(self.config.checkpoints, setting)\n",
    "    \n",
    "    # Metric trackers (updated per epoch; best tracked by validation loss)\n",
    "    all_epoch_metrics = []\n",
    "    best_metrics = {\n",
    "        \"epoch\": 0,\n",
    "        \"train_loss\": float('inf'),\n",
    "        \"vali_loss\": float('inf'),\n",
    "        \"vali_mae_loss\": float('inf'),\n",
    "        \"test_loss\": float('inf'),\n",
    "        \"test_mae_loss\": float('inf')\n",
    "    }\n",
    "    best_model_path = \"\"\n",
    "    \n",
    "    time_now = time.time()\n",
    "    train_steps = len(train_loader)\n",
    "\n",
    "    # Early stopping with accelerator-aware checkpointing\n",
    "    early_stopping = EarlyStopping(patience=self.config.patience, verbose=True, accelerator=self.accelerator)\n",
    "    \n",
    "    # Prepare model, optimizer, and loaders for device/distributed execution\n",
    "    self.model, self.optimizer, train_loader, vali_loader, test_loader = self.accelerator.prepare(\n",
    "        self.model, self.optimizer, train_loader, vali_loader, test_loader\n",
    "    )\n",
    "    \n",
    "    # ===== Main training loop =====\n",
    "    for epoch in range(self.config.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "        \n",
    "        self.model.train()\n",
    "        epoch_time = time.time()\n",
    "        batch_times = []  # Per-batch wall-clock timing\n",
    "        \n",
    "        # ----- Batch loop -----\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()\n",
    "            iter_count += 1\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Move inputs to device (keep dtypes explicit)\n",
    "            batch_x = batch_x.float().to(self.device)\n",
    "            batch_y = batch_y.float().to(self.device)\n",
    "            batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "            \n",
    "            # Decoder input: teacher-forcing prefix + zeros for prediction horizon\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -self.config.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :self.config.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "            \n",
    "            # Forward with mixed precision (managed by Accelerator)\n",
    "            with self.accelerator.autocast():\n",
    "                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            \n",
    "            # Some models return (pred, aux_loss); handle both cases\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, additional_loss = outputs\n",
    "            else:\n",
    "                additional_loss = 0\n",
    "            \n",
    "            # Compute loss on prediction horizon only\n",
    "            batch_y = batch_y[:, -self.config.pred_len:, :].to(self.device)\n",
    "            loss = self.criterion(outputs, batch_y) + additional_loss\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            # Periodic logging with speed estimate\n",
    "            if (i + 1) % 100 == 0:\n",
    "                self.accelerator.print(f\"\\titers: {i+1}, epoch: {epoch+1} | loss: {loss.item():.7f}\")\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((self.config.train_epochs - epoch) * train_steps - i)\n",
    "                self.accelerator.print(f'\\tspeed: {speed:.4f}s/iter; left time: {left_time:.4f}s')\n",
    "                iter_count = 0\n",
    "                time_now = time.time()\n",
    "            \n",
    "            # Backward + optimizer step (scaled if AMP is on)\n",
    "            self.accelerator.backward(loss)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            batch_times.append(time.time() - batch_start_time)\n",
    "        # ----- End batch loop -----\n",
    "        \n",
    "        # Timing diagnostics\n",
    "        epoch_cost_time = time.time() - epoch_time\n",
    "        avg_batch_time = np.mean(batch_times)\n",
    "        self.accelerator.print(f\"Epoch: {epoch+1} cost time: {epoch_cost_time:.2f}s\")\n",
    "        self.accelerator.print(f\"Average batch training time: {avg_batch_time:.4f}s\")\n",
    "        \n",
    "        # Evaluation on validation and test (no gradient)\n",
    "        train_loss = np.average(train_loss)\n",
    "        val_time = time.time()\n",
    "        vali_loss, vali_mae_loss = self.validate(vali_loader)\n",
    "        self.accelerator.print(f\"Val cost time: {time.time() - val_time:.2f}s\")\n",
    "        test_time = time.time()\n",
    "        test_loss, test_mae_loss = self.validate(test_loader)\n",
    "        self.accelerator.print(f\"Test cost time: {time.time() - test_time:.2f}s\")\n",
    "        \n",
    "        # Log epoch metrics and progress\n",
    "        epoch_metrics = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"vali_loss\": float(vali_loss),\n",
    "            \"vali_mae_loss\": float(vali_mae_loss),\n",
    "            \"test_loss\": float(test_loss),\n",
    "            \"test_mae_loss\": float(test_mae_loss)\n",
    "        }\n",
    "        all_epoch_metrics.append(epoch_metrics)\n",
    "        self.accelerator.print(f'Epoch: {epoch+1}, Steps: {train_steps} | Train Loss: {train_loss:.7f} Vali Loss: {vali_loss:.7f} Test Loss: {test_loss:.7f}')\n",
    "        \n",
    "        # Early stopping (saves checkpoint on improvement)\n",
    "        early_stopping(vali_loss, self.model, path, metrics=epoch_metrics)\n",
    "\n",
    "        # Track best-by-validation-loss and checkpoint path\n",
    "        if vali_loss < best_metrics[\"vali_loss\"]:\n",
    "            best_metrics.update(epoch_metrics)\n",
    "            best_model_path = early_stopping.get_checkpoint_path()\n",
    "\n",
    "        # Stop training if patience is exceeded\n",
    "        if early_stopping.early_stop:\n",
    "            self.accelerator.print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        # Scheduler step (epoch-based policy)\n",
    "        adjust_learning_rate(self.optimizer, epoch + 1, self.config, self.accelerator)\n",
    "    \n",
    "    return all_epoch_metrics, best_metrics, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more, please see it at core/experiments/long_term_forecasting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Wrap model, optimizer, and dataloaders with Accelerator for device placement and distributed training\n",
    "self.model, self.optimizer, train_loader, vali_loader, test_loader = self.accelerator.prepare(\n",
    "    self.model, self.optimizer, train_loader, vali_loader, test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, accelerator=None):\n",
    "        # Number of epochs to wait before stopping if no improvement\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        # Minimum improvement in validation loss to reset counter\n",
    "        self.delta = delta\n",
    "        # Optional: use accelerator for printing/logging\n",
    "        self.accelerator = accelerator\n",
    "        # Store best validation metrics (for checkpointing/analysis)\n",
    "        self.best_metrics = None  \n",
    "\n",
    "    def __call__(self, val_loss, model, path, metrics=None):\n",
    "        # Convert validation loss into score (lower loss â†’ higher score)\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            # First evaluation: save as baseline best\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path, metrics)\n",
    "            if metrics is not None:\n",
    "                self.best_metrics = metrics\n",
    "        elif score < self.best_score + self.delta:\n",
    "            # No sufficient improvement â†’ increase counter\n",
    "            self.counter += 1\n",
    "            if self.accelerator:\n",
    "                self.accelerator.print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            else:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            # Stop if patience exceeded\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # Improvement found â†’ save checkpoint and reset counter\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path, metrics)\n",
    "            if metrics is not None:\n",
    "                self.best_metrics = metrics\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, args, accelerator=None):\n",
    "    \"\"\"\n",
    "    Adjust the learning rate according to different scheduling strategies.\n",
    "    \n",
    "    Supported strategies are selected by args.lradj.\n",
    "    Updates optimizer.param_groups['lr'] if current epoch matches schedule.\n",
    "    \"\"\"\n",
    "    if args.lradj == 'type1':\n",
    "        # Halve LR every epoch\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        # Manually specified step decay\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif args.lradj == 'type3':\n",
    "        # Keep constant LR for first 3 epochs, then exponential decay\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 3 \n",
    "                     else args.learning_rate * (0.9 ** ((epoch - 3) // 1))}\n",
    "    elif args.lradj == 'constant':\n",
    "        # Fixed learning rate\n",
    "        lr_adjust = {epoch: args.learning_rate}\n",
    "    elif args.lradj == 'TSLR':\n",
    "        # Slowly decaying LR over long horizon\n",
    "        lr_adjust = {epoch: args.learning_rate * ((0.5 ** 0.1) ** (epoch // 20))}\n",
    "    elif args.lradj == '3':\n",
    "        # Step decay after 10 epochs\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate * 0.1}\n",
    "    elif args.lradj == '4':\n",
    "        # Step decay after 15 epochs\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate * 0.1}\n",
    "    elif args.lradj == '5':\n",
    "        # Step decay after 25 epochs\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate * 0.1}\n",
    "    elif args.lradj == '6':\n",
    "        # Step decay after 5 epochs\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate * 0.1}\n",
    "    elif args.lradj == 'TST':\n",
    "        # Linear warm-up style schedule (increasing slightly each epoch)\n",
    "        lr_adjust = {epoch: args.learning_rate * (1.0 + 0.1 * epoch / args.train_epochs)}\n",
    "\n",
    "    # Apply LR update if schedule defines current epoch\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        if accelerator is not None:\n",
    "            accelerator.print(f'Updating learning rate to {lr}')\n",
    "        else:\n",
    "            print(f'Updating learning rate to {lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def validate(self, vali_loader=None) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Validate the model with distributed-safe metric aggregation.\n",
    "\n",
    "    Uses explicit accumulation of squared and absolute errors to avoid\n",
    "    GPU out-of-memory during evaluation.\n",
    "\n",
    "    Args:\n",
    "        vali_loader: Optional validation DataLoader. If None, a loader is created.\n",
    "\n",
    "    Returns:\n",
    "        (MSE, MAE) as floats\n",
    "    \"\"\"\n",
    "    if vali_loader is None:\n",
    "        _, vali_loader = self._get_data(flag='val')\n",
    "\n",
    "    # Initialize accumulators on device\n",
    "    sum_sq_error = torch.tensor(0.0, device=self.device)\n",
    "    sum_abs_error = torch.tensor(0.0, device=self.device)\n",
    "    total_count = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in vali_loader:\n",
    "            # Move batch to device\n",
    "            batch_x = batch_x.float().to(self.device)\n",
    "            batch_y = batch_y.float().to(self.device)\n",
    "            batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "            # Decoder input: label_len history + zero padding for pred_len\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -self.config.pred_len:, :])\n",
    "            dec_inp = torch.cat(\n",
    "                [batch_y[:, :self.config.label_len, :], dec_inp], dim=1\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Forward pass (supports AMP and tuple outputs)\n",
    "            with self.accelerator.autocast():\n",
    "                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            # Target slice: prediction horizon only\n",
    "            true_slice = batch_y[:, -self.config.pred_len:, :]\n",
    "\n",
    "            # Accumulate squared and absolute errors\n",
    "            error = outputs - true_slice\n",
    "            sum_sq_error += error.pow(2).sum()\n",
    "            sum_abs_error += error.abs().sum()\n",
    "            total_count += torch.tensor(error.numel(), device=self.device)\n",
    "\n",
    "    # Distributed reduction: aggregate across all processes\n",
    "    sum_sq_error = self.accelerator.reduce(sum_sq_error, reduction=\"sum\")\n",
    "    sum_abs_error = self.accelerator.reduce(sum_abs_error, reduction=\"sum\")\n",
    "    total_count = self.accelerator.reduce(total_count, reduction=\"sum\")\n",
    "\n",
    "    # Final metrics\n",
    "    mse = sum_sq_error / total_count\n",
    "    mae = sum_abs_error / total_count\n",
    "\n",
    "    self.model.train()\n",
    "    return mse.item(), mae.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test(self, setting: str, best_model_path: Optional[str] = None) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on test data using the same aggregation logic as in validate().\n",
    "\n",
    "    Args:\n",
    "        setting: Experiment identifier string, used for result saving and fallback checkpoint loading.\n",
    "        best_model_path: Path to the model checkpoint. If None, defaults to ./checkpoints/{setting}.pth\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (MSE, MAE) on the test set.\n",
    "    \"\"\"\n",
    "    # Select checkpoint (default path if none provided)\n",
    "    if best_model_path is None:\n",
    "        best_model_path = os.path.join(self.config.checkpoints, f\"{setting}.pth\")\n",
    "\n",
    "    self.accelerator.print(f'Loading trained model {best_model_path} for testing')\n",
    "    \n",
    "    # Load best model weights (unwrap to avoid DDP/AMP wrappers)\n",
    "    self.model = self.accelerator.unwrap_model(self.model)\n",
    "    self.model.load_state_dict(torch.load(best_model_path, map_location='cpu'))\n",
    "\n",
    "    # Build test loader and prepare for distributed inference\n",
    "    _, test_loader = self._get_data(flag='test')\n",
    "    self.model, test_loader = self.accelerator.prepare(self.model, test_loader)\n",
    "\n",
    "    # Reuse validate() to compute final metrics\n",
    "    mse, mae = self.validate(test_loader)\n",
    "\n",
    "    self.accelerator.print(f'Test MSE: {mse:.6f}, Test MAE: {mae:.6f}')\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Prepare datasets from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the Time-HD benchmark dataset, follow these steps:\n",
    "\n",
    "a. Create a Hugging Face account, if you do not already have one.\n",
    "\n",
    "b. Visit the dataset page:  \n",
    "   [https://huggingface.co/datasets/Time-HD-Anonymous/High_Dimensional_Time_Series](https://huggingface.co/datasets/Time-HD-Anonymous/High_Dimensional_Time_Series)\n",
    "\n",
    "c. Click **\"Agree and access repository\"**. You must be logged in to complete this step.\n",
    "\n",
    "d. Create new Access Token. Token type should be \"write\".\n",
    "\n",
    "e. Authenticate on your local machine by running:\n",
    "\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "\n",
    "   and enter your generated token above.\n",
    "\n",
    "f. Then, you can manually download all the dataset by running:\n",
    "\n",
    "   ```bash\n",
    "   python download_dataset.py\n",
    "   ```\n",
    "\n",
    "The summary of the supported high-dimensional time series datasets is shown in Table 2 above. Besides these, we also support datasets such as ECL, ETTh1, ETTh2, ETTm1, ETTm2, Weather, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Running the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# ðŸ–¥ï¸ Single GPU training\n",
    "accelerate launch --num_processes=1 run.py --model UCast --data \"Measles\" --gpu 0\n",
    "\n",
    "# ðŸš€ Multi-GPU training (auto-detect all GPUs)\n",
    "accelerate launch run.py --model UCast --data \"Measles\"\n",
    "\n",
    "# ðŸŽ¯ Specific GPU selection (e.g. 4 GPUs, id: 0,2,3,7)\n",
    "accelerate launch --num_processes=4 run.py --model UCast --data \"Measles\" --gpu 0,2,3,7\n",
    "\n",
    "# ðŸ“‹ List available models\n",
    "accelerate launch run.py --list-models\n",
    "\n",
    "# â„¹ï¸ Show framework information\n",
    "python run.py --info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sign of a successful experiment running is that information about the experiment is printed out, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "================================================================================\n",
    "ðŸš€ Time-HD-Lib: A Lirbrary for High-Dimensional Time Series Forecasting\n",
    "================================================================================\n",
    "Loaded default parameters for Measles from configs/UCast.yaml: {'enc_in': 1161, 'train_epochs': 100, 'alpha': 0.001, 'seq_len_factor': 3, 'learning_rate': 0.0005, 'd_model': 512, 'e_layers': 2, 'lradj': 'type3', 'batch_size': 32}\n",
    "args.batchsize: 32\n",
    "\n",
    "ðŸ“‹ Configuration Summary:\n",
    "   Task: long_term_forecast\n",
    "   Model: UCast\n",
    "   Dataset: Measles\n",
    "   Input Sequence Length: 21\n",
    "   Prediction Length: 7\n",
    "   Training Mode: Yes\n",
    "   GPU: Yes\n",
    "\n",
    "ðŸ”§ Initializing experiment runner...\n",
    "ðŸŽ¯ Starting experiment execution...\n",
    "\n",
    "=== Accelerator Device Information ===\n",
    "Number of processes: 8\n",
    "Distributed type: MULTI_GPU\n",
    "\n",
    "Device details for all processes:\n",
    "  GPU #0: NVIDIA L40S - Total memory: 44.40 GB\n",
    "  GPU #1: NVIDIA L40S - Total memory: 44.40 GB\n",
    "  GPU #2: NVIDIA L40S - Total memory: 44.40 GB\n",
    "  GPU #3: NVIDIA L40S - Total memory: 44.40 GB\n",
    "  GPU #4: NVIDIA L40S - Total memory: 44.40 GB\n",
    "  GPU #5: NVIDIA L40S - Total memory: 44.40 GB\n",
    "  GPU #6: NVIDIA L40S - Total memory: 44.40 GB\n",
    "  GPU #7: NVIDIA L40S - Total memory: 44.40 GB\n",
    "=======================================\n",
    "\n",
    ">>> Starting training for long_term_forecast_UCast_Measles_sl21_pl7 <<<\n",
    "train 903\n",
    "val 128\n",
    "test 260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the model starts training. Once one epoch finishes training, information like below will be printer outï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Epoch: 1 cost time: 0.94s\n",
    "Average batch training time: 0.1826s\n",
    "Val cost time: 0.21s\n",
    "Test cost time: 0.17s\n",
    "Epoch: 1, Steps: 29 | Train Loss: 0.9740449 Vali Loss: 0.0712604 Test Loss: 0.0255510\n",
    "Validation loss decreased (inf --> 0.071260).  Saving model ...\n",
    "Updating learning rate to 0.0005\n",
    "...\n",
    "Epoch: 40 cost time: 0.34s\n",
    "Average batch training time: 0.0256s\n",
    "Val cost time: 0.21s\n",
    "Test cost time: 0.18s\n",
    "Epoch: 40, Steps: 29 | Train Loss: 0.3438028 Vali Loss: 0.0463830 Test Loss: 0.0147475\n",
    "EarlyStopping counter: 3 out of 3\n",
    "Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all epochs are done, the model steps into testing. The following information about testing will be printed out, giving the MAE and MSE of test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    ">>> Starting testing for long_term_forecast_UCast_Measles_sl21_pl7 <<<\n",
    "Loading trained model ./checkpoints/long_term_forecast_UCast_Measles_sl21_pl7.pth for testing\n",
    "test 260\n",
    "Test MSE: 0.014731, Test MAE: 0.052162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "================================================================================\n",
    "âœ… Experiment Completed Successfully!\n",
    "================================================================================\n",
    "\n",
    "ðŸ“Š Best Training Results (Epoch 37):\n",
    "   Train Loss: 0.349271\n",
    "   Validation Loss: 0.046337\n",
    "   Validation MAE: 0.108613\n",
    "   Test Loss: 0.014731\n",
    "   Test MAE: 0.052162\n",
    "\n",
    "ðŸŽ¯ Final Test Results:\n",
    "   MSE: 0.014731\n",
    "   MAE: 0.052162\n",
    "\n",
    "ðŸŽ‰ All results have been saved to the experiments directory."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
